install.packages("languageserver")
q()
q()
source("C:/Users/vaibh/Downloads/CMPT 318/Ass2/ans.r")
setwd("C:/Users/vaibh/Downloads/STAT 240/lab08")
# create a character vector urls to store URLS
urls <- c()
# read in html file
html <- readLines("./cbc.ca.2023.03.23.html", warn=FALSE)
html
# read in html file
html_doc <- readLines("./cbc.ca.2023.03.23.html", warn=FALSE)
length(html)
# loop thru the doc
for (i in 1:length(html_doc)) {
# find the line with the URL
if (grepl("href", html_doc[i])) {
# extract the URL
url <- gsub(".*href=\"", "", html_doc[i])
url <- gsub("\".*", "", url)
# check if the URL is http or https
if (grepl("http", url)) {
# check if the URL is already in the vector
if (!url %in% urls) {
# add the URL to the vector
urls <- c(urls, url)
}
}
}
}
# create a character vector urls to store URLS
urls <- c()
# read in html file
html_doc <- readLines("./cbc.ca.2023.03.23.html", warn=FALSE)
# loop thru the doc
for (i in 1:length(html_doc)) {
# find the line with the URL
if (grepl("href", html_doc[i])) {
# extract the URL
url <- gsub(".*href=\"", "", html_doc[i])
url <- gsub("\".*", "", url)
# check if the URL is http or https
if (grepl("http", url)) {
# add the URL to the vector
urls <- c(urls, url)
}
}
}
# print the length of the URLs array
print(length(urls))
urls
# create a character vector urls to store URLS
urls <- c()
# read the HTML file into a string
html_string <- readLines("cbc.ca.2023.03.23.html", warn = FALSE)
# extract URLs using regular expressions
pattern <- "((http|https)://[[:graph:]]+)[[:space:]'\"]"
urls <- unlist(regmatches(html_string, gregexpr(pattern, html_string)))
# remove trailing whitespace and quotes from URLs
urls <- gsub("[[:space:]'\"]$", "", urls)
# print the resulting URLs
print(urls)
# extract URLs using regular expressions
pattern <- "((http|https)://[[:graph:]]+)[[:space:]'\"\\]"
urls <- unlist(regmatches(html_string, gregexpr(pattern, html_string)))
# remove trailing whitespace and quotes from URLs
urls <- gsub("[[:space:]'\"]$", "", urls)
# print the resulting URLs
print(urls)
# remove trailing whitespace and quotes from URLs
urls <- gsub("[[:space:]\\'\"]$", "", urls)
# print the resulting URLs
print(urls)
# define updated regular expression pattern to match URLs
pattern <- "(https?://[^\\s\"'\\\\]*(?:\\\\.[^\\s\"'\\\\]*)*)"
# extract URLs using regular expression matching
urls <- regmatches(html_string, gregexpr(pattern, html_string, ignore.case = TRUE))[[1]]
# remove quotes or whitespace at the end of URLs
urls <- gsub("[\"'\\s]+$", "", urls)
# print resulting URLs
print(urls)
# create a character vector urls to store URLS
urls <- c()
# read the HTML file into a string
html_string <- readLines("cbc.ca.2023.03.23.html", warn = FALSE)
# loop thru the doc
for (i in 1:length(html_string)) {
# find the line with the URL
if (grepl("href", html_string[i])) {
# extract the URL
url <- gsub(".*href=\"", "", html_string[i])
url <- gsub("\".*", "", url)
# check if the URL is http or https
if (grepl("http", url)) {
# check if the URL is already in the vector
if (!url %in% urls) {
# add the URL to the vector
urls <- c(urls, url)
}
}
}
}
# extract URLs using regular expressions
pattern <- "((http|https)://[[:graph:]]+)[[:space:]'\"\\]"
urls <- unlist(regmatches(html_string, gregexpr(pattern, html_string)))
# remove trailing whitespace and quotes from URLs
urls <- gsub("[[:space:]\\'\"]$", "", urls)
# print the length of resulting URLs
sprintf("The no of urls in the HTML are: %i", urls.length)
# create a character vector urls to store URLS
urls <- c()
# read the HTML file into a string
html_string <- readLines("cbc.ca.2023.03.23.html", warn = FALSE)
# loop thru the doc
for (i in 1:length(html_string)) {
# find the line with the URL
if (grepl("href", html_string[i])) {
# extract the URL
url <- gsub(".*href=\"", "", html_string[i])
url <- gsub("\".*", "", url)
# check if the URL is http or https
if (grepl("http", url)) {
# check if the URL is already in the vector
if (!url %in% urls) {
# add the URL to the vector
urls <- c(urls, url)
}
}
}
}
# extract URLs using regular expressions
pattern <- "((http|https)://[[:graph:]]+)[[:space:]'\"\\]"
urls <- unlist(regmatches(html_string, gregexpr(pattern, html_string)))
# remove trailing whitespace and quotes from URLs
urls <- gsub("[[:space:]\\'\"]$", "", urls)
# print the length of resulting URLs
sprintf("The no of urls in the HTML are: %i",  length(urls))
